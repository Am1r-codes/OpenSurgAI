"""Phase-centric explanation generation with Nemotron system commentary.

Architecture
------------
Two-part explanation model:

**Part A — Canonical phase knowledge (static, hard-coded)**
    Fixed, expert-written educational descriptions for each Cholec80
    surgical phase.  These are deterministic, never generated by an LLM,
    and reused across all frames and videos.

**Part B — Nemotron system commentary (dynamic, optional)**
    A single sentence of system-level context generated by Nemotron.
    Nemotron reasons ONLY about system state (confidence, transitions,
    stability).  It must NOT describe anatomy, instruments, or surgical
    actions.

Trigger logic
-------------
Nemotron is called ONLY when:
- ``phase_id`` changes from the previous frame, OR
- ``phase_confidence`` crosses above a configurable threshold (default
  0.95) for the first time in the current phase segment.

Between events the current explanation (canonical + commentary) persists
across all frames.

Output format (JSON Lines)::

    {
      "video_id": "video01",
      "frame_idx": 0,
      "timestamp_sec": 0.0,
      "explanation": "Initial access and setup of the operative ...",
      "system_comment": "High confidence phase transition detected.",
      "grounded": true,
      "phase_name": "Preparation",
      "trigger": "phase_change",
      "model": "nvidia/llama-3.3-nemotron-super-49b-v1.5",
      "usage": {"prompt_tokens": 80, "completion_tokens": 16}
    }
"""

from __future__ import annotations

import json
import logging
import os
import time
from dataclasses import dataclass, field
from pathlib import Path

import httpx

log = logging.getLogger(__name__)

# ── Part A: Canonical phase explanations (static, expert-written) ────

PHASE_EXPLANATIONS: dict[str, str] = {
    "Preparation": (
        "Initial access and setup of the operative field. The abdomen is "
        "insufflated, trocars are placed, and the gallbladder is exposed "
        "to establish orientation."
    ),
    "CalotTriangleDissection": (
        "Dissection is focused on the Calot triangle to identify and "
        "expose the cystic duct and cystic artery. This phase is critical "
        "to prevent bile duct injury."
    ),
    "ClippingCutting": (
        "Once structures are clearly identified, the cystic duct and "
        "artery are clipped and divided. Precision is essential to avoid "
        "bleeding or bile leakage."
    ),
    "GallbladderDissection": (
        "The gallbladder is separated from the liver bed using blunt and "
        "electrocautery dissection, progressing from the infundibulum "
        "toward the fundus."
    ),
    "GallbladderPackaging": (
        "The gallbladder is placed into a retrieval bag in preparation "
        "for removal from the abdominal cavity."
    ),
    "CleaningCoagulation": (
        "Hemostasis is performed. Bleeding points are coagulated and the "
        "surgical field is inspected for bile leakage or injury."
    ),
    "GallbladderRetraction": (
        "The gallbladder is retracted to improve visualization and access "
        "during earlier dissection phases."
    ),
}

# ── Part B: System prompt for Nemotron commentary ────────────────────

SYSTEM_PROMPT = """\
You are a system status reporter for a computer-assisted surgery platform.
You receive the current surgical phase name, the model's confidence score,
and whether this is a phase transition or a stable continuation.

YOUR TASK: produce exactly ONE short sentence (max 15 words) describing
the system's state.

ALLOWED outputs (examples):
- "High confidence phase transition detected."
- "The system is confident the procedure has entered a new phase."
- "Phase recognition is stable with high confidence."
- "Transition detected; confidence is building."
- "The current phase remains stable."

STRICT RULES:
1. Do NOT mention anatomy, organs, instruments, or surgical structures.
2. Do NOT teach or explain the surgical procedure.
3. Do NOT reference pixels, detections, masks, bounding boxes, or frames.
4. Do NOT use more than one sentence.
5. Speak about the SYSTEM and its CONFIDENCE, not the surgery itself.
6. Return ONLY the sentence, no prefixes, labels, or quotes.\
"""


# ── Data classes ──────────────────────────────────────────────────────

@dataclass
class ExplanationResult:
    """Combined canonical explanation + system commentary for a frame."""
    video_id: str
    frame_idx: int
    timestamp_sec: float
    explanation: str
    system_comment: str
    grounded: bool
    model: str
    phase_name: str = ""
    trigger: str = ""
    usage: dict = field(default_factory=dict)

    def to_dict(self) -> dict:
        return {
            "video_id": self.video_id,
            "frame_idx": self.frame_idx,
            "timestamp_sec": round(self.timestamp_sec, 4),
            "explanation": self.explanation,
            "system_comment": self.system_comment,
            "grounded": self.grounded,
            "phase_name": self.phase_name,
            "trigger": self.trigger,
            "model": self.model,
            "usage": self.usage,
        }

    def to_json(self) -> str:
        return json.dumps(self.to_dict())


# ── Grounding check ──────────────────────────────────────────────────

def _check_grounding(system_comment: str, phase_name: str) -> bool:
    """Verify the system comment does not leak surgical content."""
    text_lower = system_comment.lower()
    # Must not mention specific phase names (only the system should talk about itself)
    surgical_terms = {
        "calot", "cystic", "gallbladder", "liver", "artery", "duct",
        "trocar", "insufflat", "coagulat", "hemostasis", "clipper",
        "grasper", "hook", "scissors", "bipolar", "irrigator",
        "specimenbag", "specimen bag",
    }
    for term in surgical_terms:
        if term in text_lower:
            log.debug("Grounding fail: surgical term '%s' in system comment", term)
            return False
    return True


# ── Nemotron API client ──────────────────────────────────────────────

class NemotronClient:
    """Thin wrapper around the Nemotron / NVIDIA NIM chat completions API."""

    def __init__(
        self,
        api_key: str | None = None,
        base_url: str = "https://integrate.api.nvidia.com/v1",
        model: str = "nvidia/llama-3.3-nemotron-super-49b-v1.5",
        temperature: float = 0.2,
        max_tokens: int | None = 64,
        top_p: float | None = None,
        presence_penalty: float | None = None,
        timeout: float = 60.0,
        max_retries: int = 3,
    ) -> None:
        self.api_key = api_key or os.environ.get("NEMOTRON_API_KEY") or os.environ.get("NVIDIA_API_KEY")
        if not self.api_key:
            raise ValueError(
                "Nemotron API key required. Set NEMOTRON_API_KEY or "
                "NVIDIA_API_KEY environment variable, or pass api_key=."
            )
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.top_p = top_p
        self.presence_penalty = presence_penalty
        self.timeout = timeout
        self.max_retries = max_retries
        self._client = httpx.Client(timeout=self.timeout)

    def chat(self, system: str, user: str) -> dict:
        """Send a chat completion request."""
        url = f"{self.base_url}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        payload: dict = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            "temperature": self.temperature,
        }
        if self.max_tokens is not None:
            payload["max_tokens"] = self.max_tokens
        if self.top_p is not None:
            payload["top_p"] = self.top_p
        if self.presence_penalty is not None:
            payload["presence_penalty"] = self.presence_penalty

        last_exc: Exception | None = None
        for attempt in range(1, self.max_retries + 1):
            try:
                resp = self._client.post(url, headers=headers, json=payload)
                if resp.status_code == 200:
                    return resp.json()
                if resp.status_code == 429 or resp.status_code >= 500:
                    wait = min(2 ** attempt, 30)
                    log.warning(
                        "API %d on attempt %d/%d – retrying in %ds",
                        resp.status_code, attempt, self.max_retries, wait,
                    )
                    time.sleep(wait)
                    continue
                resp.raise_for_status()
            except httpx.TimeoutException as exc:
                last_exc = exc
                wait = min(2 ** attempt, 30)
                log.warning(
                    "Timeout on attempt %d/%d – retrying in %ds",
                    attempt, self.max_retries, wait,
                )
                time.sleep(wait)
            except httpx.HTTPStatusError as exc:
                raise RuntimeError(
                    f"Nemotron API error {exc.response.status_code}: "
                    f"{exc.response.text}"
                ) from exc

        raise RuntimeError(
            f"Nemotron API failed after {self.max_retries} retries"
            + (f": {last_exc}" if last_exc else "")
        )

    def close(self) -> None:
        self._client.close()

    def __enter__(self) -> NemotronClient:
        return self

    def __exit__(self, *exc) -> None:
        self.close()


# ── Explanation pipeline ──────────────────────────────────────────────

class ExplanationPipeline:
    """Two-part explanation generator: canonical knowledge + system commentary.

    Part A (canonical) is always available, with zero API calls.
    Part B (Nemotron) fires only on phase transitions or confidence events.
    """

    def __init__(
        self,
        api_key: str | None = None,
        base_url: str = "https://integrate.api.nvidia.com/v1",
        model: str = "nvidia/llama-3.3-nemotron-super-49b-v1.5",
        temperature: float = 0.2,
        max_tokens: int = 64,
        chunk_size: int = 5,
        system_prompt: str | None = None,
        confidence_threshold: float = 0.95,
    ) -> None:
        self.model_name = model
        self.chunk_size = max(1, chunk_size)
        self.system_prompt = system_prompt or SYSTEM_PROMPT
        self.confidence_threshold = confidence_threshold

        self.client = NemotronClient(
            api_key=api_key,
            base_url=base_url,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        log.info(
            "ExplanationPipeline ready  (model=%s, confidence_threshold=%.2f)",
            model, confidence_threshold,
        )

        # Cache: (phase_name, trigger_type) -> (comment, usage)
        self._comment_cache: dict[str, tuple[str, dict]] = {}

    # ── Part B: get system commentary from Nemotron ───────────────────

    def _get_system_comment(
        self,
        phase_name: str,
        confidence: float,
        trigger: str,
    ) -> tuple[str, dict]:
        """Generate a one-sentence system comment via Nemotron.

        Returns (comment_text, usage_dict).  Results are cached per
        (phase_name, trigger) so repeat transitions reuse the same call.
        """
        cache_key = f"{phase_name}:{trigger}"
        if cache_key in self._comment_cache:
            return self._comment_cache[cache_key]

        if trigger == "phase_change":
            context = "This is a NEW phase transition."
        else:
            context = "The phase is continuing with newly high confidence."

        user_prompt = (
            f"Phase: {phase_name}\n"
            f"Confidence: {confidence:.2f}\n"
            f"Context: {context}"
        )

        fallback = (
            "Phase transition detected." if trigger == "phase_change"
            else "Phase recognition is stable with high confidence."
        )

        try:
            response = self.client.chat(
                system=self.system_prompt,
                user=user_prompt,
            )
            choices = response.get("choices", [])
            text = choices[0]["message"]["content"].strip() if choices else fallback
            usage = response.get("usage", {})
        except Exception as exc:
            log.warning("Nemotron call failed, using fallback: %s", exc)
            text = fallback
            usage = {}

        # Validate: must not contain surgical content
        if not _check_grounding(text, phase_name):
            log.warning("System comment leaked surgical content, using fallback")
            text = fallback

        self._comment_cache[cache_key] = (text, usage)
        log.info("System comment for %s [%s]: %s", phase_name, trigger, text)
        return text, usage

    # ── event-driven processing ───────────────────────────────────────

    def process_scenes(
        self, scenes: list[dict]
    ) -> list[ExplanationResult]:
        """Generate explanations driven by phase changes / confidence events.

        Part A (canonical) is always set from PHASE_EXPLANATIONS.
        Part B (system comment) is generated only on trigger events.
        """
        results: list[ExplanationResult] = []
        if not scenes:
            return results

        current_phase: str | None = None
        current_explanation = ""
        current_comment = ""
        current_usage: dict = {}
        high_conf_explained: set[str] = set()
        api_calls = 0

        for scene in scenes:
            phase = scene.get("phase", {})
            phase_name = phase.get("phase_name", "")
            confidence = phase.get("confidence", 0.0)
            video_id = scene.get("video_id", "")
            frame_idx = scene.get("frame_idx", 0)
            timestamp = scene.get("timestamp_sec", 0.0)

            trigger = ""

            # Trigger 1: phase changed
            if phase_name != current_phase:
                current_explanation = PHASE_EXPLANATIONS.get(phase_name, "")
                current_comment, current_usage = self._get_system_comment(
                    phase_name, confidence, "phase_change",
                )
                current_phase = phase_name
                trigger = "phase_change"
                high_conf_explained.discard(phase_name)
                api_calls += 1
                log.info(
                    "Phase change at frame %d: %s (conf=%.3f)",
                    frame_idx, phase_name, confidence,
                )

            # Trigger 2: confidence crossed threshold (first time)
            elif (
                confidence >= self.confidence_threshold
                and phase_name not in high_conf_explained
            ):
                current_explanation = PHASE_EXPLANATIONS.get(phase_name, "")
                current_comment, current_usage = self._get_system_comment(
                    phase_name, confidence, "high_confidence",
                )
                high_conf_explained.add(phase_name)
                trigger = "high_confidence"
                api_calls += 1
                log.info(
                    "High confidence (%.3f) at frame %d: %s",
                    confidence, frame_idx, phase_name,
                )

            grounded = _check_grounding(current_comment, phase_name)

            results.append(ExplanationResult(
                video_id=video_id,
                frame_idx=frame_idx,
                timestamp_sec=timestamp,
                explanation=current_explanation,
                system_comment=current_comment,
                grounded=grounded,
                model=self.model_name,
                phase_name=phase_name,
                trigger=trigger,
                usage=current_usage if trigger else {},
            ))

        log.info(
            "Processed %d frames, %d API calls (cached %d comments)",
            len(scenes), api_calls, len(self._comment_cache),
        )
        return results

    # ── convenience: JSONL in -> JSONL out ────────────────────────────

    def process_scene_file(
        self,
        scene_path: Path,
        output_path: Path,
        max_frames: int | None = None,
    ) -> dict:
        """Read a scene JSONL, generate explanations, write output JSONL."""
        scene_path = Path(scene_path)
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        scenes: list[dict] = []
        with open(scene_path, encoding="utf-8") as fh:
            for line in fh:
                line = line.strip()
                if line:
                    scenes.append(json.loads(line))

        if max_frames is not None and max_frames > 0:
            scenes = scenes[:max_frames]

        video_id = scenes[0].get("video_id", scene_path.stem) if scenes else ""
        log.info("Generating explanations for %s (%d frames)", video_id, len(scenes))

        t0 = time.perf_counter()
        results = self.process_scenes(scenes)

        with open(output_path, "w", encoding="utf-8") as fh:
            for r in results:
                fh.write(r.to_json() + "\n")

        elapsed = time.perf_counter() - t0
        n_grounded = sum(1 for r in results if r.grounded)
        total_prompt = sum(r.usage.get("prompt_tokens", 0) for r in results)
        total_completion = sum(r.usage.get("completion_tokens", 0) for r in results)

        summary = {
            "video_id": video_id,
            "total_frames": len(results),
            "grounded_frames": n_grounded,
            "grounded_pct": round(n_grounded / len(results) * 100, 1) if results else 0,
            "total_prompt_tokens": total_prompt,
            "total_completion_tokens": total_completion,
            "elapsed_sec": round(elapsed, 2),
            "model": self.model_name,
        }
        log.info("Saved %s  (%s)", output_path, summary)
        return summary
